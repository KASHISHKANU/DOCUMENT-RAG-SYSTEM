# ğŸš€ Policy Document RAG Assistant

AI Engineer Intern â€“ Take Home Assignment

# ğŸ“Œ Overview
This project implements a Retrieval-Augmented Generation (RAG) based question-answering assistant that answers user queries using internal company policy documents.

# The goal is to demonstrate:
- Strong prompt engineering
- Proper RAG architecture
- Grounded, hallucination-free answers
- Clear reasoning and evaluation

The assistant retrieves relevant policy information and generates answers strictly grounded in retrieved context.

# ğŸ¯ Problem Statement

Given a set of company policy documents (Refund, Cancellation, Shipping policies), the system:
Retrieves relevant information from documents
Generates accurate and grounded answers
Avoids hallucinations
Uses clear and structured prompts

# ğŸ§± Architecture Overview

Policy PDFs
     â†“
PDF Loader
     â†“
Text Chunking
     â†“
Embeddings (SentenceTransformer)
     â†“
FAISS Vector Store
     â†“
Semantic Retrieval (Top-K)
     â†“
Cross-Encoder Reranking
     â†“
Prompt Template
     â†“
Gemini LLM
     â†“
Grounded Answer

# ğŸ“‚ Project Structure

rag-policy-assistant/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ refund_policy.pdf
â”‚   â”œâ”€â”€ cancellation_policy.pdf
â”‚   â””â”€â”€ shipping_policy.pdf
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

# âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone Repository
git clone <https://github.com/KASHISHKANU/DOCUMENT-RAG-SYSTEM>
cd rag-policy-assistant

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Add Gemini API Key
Create .env file:
GEMINI_API_KEY=your_api_key_here

4ï¸âƒ£ Run the Project
python src/main.py

# ğŸ“„ Dataset & Data Preparation

- Policy documents are stored as PDFs inside:
dataset/
       Data Preparation Steps
       Load PDFs using pypdf
       Extract text

- Clean and split into chunks
ğŸ”¹ Chunking Strategy
Chunk Size: 400 words
Overlap: 80 words

- Reasoning:
Keeps semantic meaning intact
Improves retrieval accuracy
Prevents context loss
Fits within LLM context window

This size provided a good balance between retrieval precision and contextual completeness.

# ğŸ§  RAG Pipeline

- Embeddings
Model: all-MiniLM-L6-v2
Generates dense semantic vectors

- Vector Store
FAISS (IndexFlatL2)
Fast similarity search
Lightweight & simple for small datasets

- Retrieval
Semantic top-K retrieval (k=3)
Retrieved chunks passed to the LLM as context

â­ Reranking (Retrieval Improvement)

After performing initial semantic retrieval using FAISS, a reranking step is applied to improve relevance before passing context to the LLM.

- Why Reranking?
Vector similarity search retrieves semantically similar chunks, but the top results are not always the most precise for the userâ€™s query.
To improve grounding quality, a second-stage reranker is used.

- Approach Used
A Cross-Encoder reranker is applied after FAISS retrieval:
FAISS retrieves Top-K relevant chunks.
Each (query, chunk) pair is scored using a cross-encoder.
Chunks are reordered based on relevance scores.
Only the highest-ranked chunks are passed to Gemini.

# âœ¨ Prompt Engineering
Prompt engineering was treated as a core focus of this assignment.

# ğŸŸ¡ Initial Prompt (Version 1)
Answer using the provided context.

Context:
{context}

Question:
{question}

- Issues Observed
Sometimes verbose
No structure
Weak hallucination control

# ğŸŸ¢ Improved Prompt (Final Version)
You are a strict company policy assistant.

RULES:
1. Answer ONLY using the given context.
2. Do NOT guess or add extra information.
3. If answer is not found, say:
   "Information not available in policy documents."
4. Use bullet points.
5. Mention source policy.

# Improvements Made

- Explicit anti-hallucination rules
- Structured output format
- Clear fallback behavior
- Better grounding to retrieved context

ğŸ§ª Evaluation
- A small evaluation set was created containing:
Answerable questions
Partially answerable questions
Unanswerable questions

# Example Evaluation Questions
Question	Type
1. How do I request a refund?	Answerable
2. Can I cancel after shipping?	Partial
3. Do you provide insurance?	Unanswerable
4. How long does delivery take?	Answerable

# Evaluation Rubric
Score	Meaning
âœ…	 Accurate & grounded
âš ï¸	  Partial / unanswerable handled correctly
âŒ	 Weak or unclear

Sample Results
Question	            Accuracy	Hallucination Avoidance	  Clarity
Refund policy	         âœ…	                âœ…	           âœ…
Cancellation	         âœ…	                âœ…	           âœ…
Insurance	              âš ï¸	                 âœ…	            âœ…

âš ï¸ Edge Case Handling
The system safely handles:

1ï¸âƒ£ No Relevant Retrieval
Returns:
No relevant policy found.

2ï¸âƒ£ Outside Knowledge Base
LLM instructed to respond:
Information not available in policy documents.
This prevents hallucination.

âš–ï¸ Key Design Trade-Offs

Why FAISS?
-Fast
-Lightweight
-Minimal setup
-Ideal for small datasets
-Why Semantic Retrieval?

Keyword search fails when wording changes.
Embeddings capture meaning rather than exact words.

Why Strict Prompting?
Prompt constraints significantly reduced hallucinations and improved grounding consistency.

ğŸ† What Iâ€™m Most Proud Of
- Clean modular RAG architecture
- Prompt iteration improving answer quality
- Grounded responses with explicit hallucination control
- Clear evaluation methodology

ğŸ”§ What I Would Improve With More Time
- Retrieval confidence scoring
- Hybrid retrieval (semantic + keyword)
- Automatic source citation mapping
- Logging & tracing for debugging
- Add Different Models and make proper pipeline and save model with best performance and use it for Answers 

ğŸ§  Tech Stack

# Core
- Python

# LLM
- Google Gemini (Gemini API) â€“ Response generation

# Retrieval & NLP
- SentenceTransformers (MiniLM) â€“ Semantic embeddings
- Cross-Encoder (MS MARCO MiniLM) â€“ Reranking for retrieval precision
- FAISS â€“ Vector similarity search

# Data Processing
- PyPDF â€“ Policy document parsing
- NumPy â€“ Vector computation

ğŸ‘¨â€ğŸ’» Author
Mr Kashish Raj